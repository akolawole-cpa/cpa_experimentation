{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What A/B testing is and when to use it\n",
    "- Key statistical concepts: null/alternative hypotheses, p-values, Type I/II errors\n",
    "- How to set up, run, and analyze a basic A/B test\n",
    "- Common pitfalls to avoid\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of statistics (mean, standard deviation)\n",
    "- Python basics\n",
    "\n",
    "## Real-World Use Cases\n",
    "- Testing a new website design to see if it increases conversions\n",
    "- Comparing two email subject lines for click-through rate\n",
    "- Evaluating whether a new product feature improves user engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is A/B Testing?\n",
    "\n",
    "**A/B testing** (also called split testing) is a method to compare two versions of something to determine which performs better.\n",
    "\n",
    "### The Basic Setup\n",
    "```\n",
    "Population of Users\n",
    "       |\n",
    "   Random Split\n",
    "      /    \\\n",
    "Control    Treatment\n",
    "  (A)         (B)\n",
    "   |           |\n",
    "Measure    Measure\n",
    "Metric     Metric\n",
    "   |           |\n",
    "   Compare Results\n",
    "```\n",
    "\n",
    "**Control (A)**: The current/baseline version  \n",
    "**Treatment (B)**: The new version you're testing\n",
    "\n",
    "### Why Randomization Matters\n",
    "Random assignment ensures the groups are comparable. Without randomization, differences in outcomes might be due to differences in the users, not the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Import libraries\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Import our experimentation library\n",
    "from experiments import ABTest, MetricType, TestType\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Key Statistical Concepts\n",
    "\n",
    "### Hypothesis Testing Framework\n",
    "\n",
    "Every A/B test is framed as a hypothesis test:\n",
    "\n",
    "- **Null Hypothesis (H₀)**: There is NO difference between A and B\n",
    "- **Alternative Hypothesis (H₁)**: There IS a difference between A and B\n",
    "\n",
    "We collect data and calculate the probability of seeing our results IF the null hypothesis were true. If this probability (p-value) is very small, we \"reject\" the null hypothesis.\n",
    "\n",
    "### Visualizing the Concept\n",
    "Imagine flipping a coin 100 times. If it's fair, you'd expect about 50 heads. But what if you got 65 heads? Is the coin biased, or did you just get lucky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sampling distribution under the null hypothesis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Simulate 10,000 experiments with a fair coin (100 flips each)\n",
    "n_simulations = 10000\n",
    "n_flips = 100\n",
    "results = np.random.binomial(n_flips, 0.5, n_simulations)  # Fair coin = 50% probability\n",
    "\n",
    "# Plot histogram\n",
    "ax.hist(results, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=50, color='green', linestyle='--', linewidth=2, label='Expected (H₀)')\n",
    "ax.axvline(x=65, color='red', linestyle='-', linewidth=2, label='Observed (65 heads)')\n",
    "\n",
    "# Shade rejection region\n",
    "x = np.linspace(0, 100, 1000)\n",
    "y = stats.binom.pmf(x.astype(int), n_flips, 0.5)\n",
    "\n",
    "ax.set_xlabel('Number of Heads', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Sampling Distribution Under Null Hypothesis (Fair Coin)', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = 2 * (1 - stats.binom.cdf(64, n_flips, 0.5))  # Two-tailed\n",
    "ax.text(75, 0.06, f'p-value = {p_value:.4f}', fontsize=12, \n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Getting 65+ or 35- heads with a fair coin has probability: {p_value:.4f}\")\n",
    "print(f\"This is {'significant' if p_value < 0.05 else 'not significant'} at the 5% level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I and Type II Errors\n",
    "\n",
    "| Decision | H₀ is True (No Effect) | H₀ is False (Real Effect) |\n",
    "|----------|------------------------|---------------------------|\n",
    "| Reject H₀ | **Type I Error** (False Positive) | Correct! |\n",
    "| Don't Reject H₀ | Correct! | **Type II Error** (False Negative) |\n",
    "\n",
    "- **α (alpha)**: Probability of Type I error (typically 0.05 or 5%)\n",
    "- **β (beta)**: Probability of Type II error\n",
    "- **Power = 1 - β**: Probability of detecting a real effect (typically 0.80 or 80%)\n",
    "\n",
    "### The Trade-off\n",
    "- Lowering α reduces false positives but increases false negatives\n",
    "- Increasing power reduces false negatives but requires larger samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Type I and Type II errors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create two distributions: null (no effect) and alternative (real effect)\n",
    "x = np.linspace(-4, 8, 1000)\n",
    "null_dist = stats.norm.pdf(x, 0, 1)  # H0: effect = 0\n",
    "alt_dist = stats.norm.pdf(x, 2.5, 1)  # H1: effect = 2.5 (true effect)\n",
    "\n",
    "# Critical value for alpha = 0.05 (one-sided)\n",
    "critical_value = stats.norm.ppf(0.95)\n",
    "\n",
    "# Plot distributions\n",
    "ax.plot(x, null_dist, 'b-', linewidth=2, label='Null (H₀): No Effect')\n",
    "ax.plot(x, alt_dist, 'g-', linewidth=2, label='Alternative (H₁): True Effect')\n",
    "\n",
    "# Shade Type I error (alpha)\n",
    "ax.fill_between(x, 0, null_dist, where=(x >= critical_value), \n",
    "                color='red', alpha=0.3, label=f'Type I Error (α = 5%)')\n",
    "\n",
    "# Shade Type II error (beta)\n",
    "ax.fill_between(x, 0, alt_dist, where=(x <= critical_value), \n",
    "                color='orange', alpha=0.3, label='Type II Error (β)')\n",
    "\n",
    "# Critical value line\n",
    "ax.axvline(x=critical_value, color='black', linestyle='--', linewidth=2, \n",
    "           label=f'Critical Value ({critical_value:.2f})')\n",
    "\n",
    "ax.set_xlabel('Test Statistic', fontsize=12)\n",
    "ax.set_ylabel('Probability Density', fontsize=12)\n",
    "ax.set_title('Type I and Type II Errors Visualized', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 0.5)\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('Reject H₀\\n(Declare winner)', xy=(3.5, 0.05), fontsize=10, ha='center')\n",
    "ax.annotate('Do not reject H₀\\n(No conclusion)', xy=(-1, 0.05), fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate power\n",
    "power = 1 - stats.norm.cdf(critical_value, 2.5, 1)\n",
    "print(f\"Power (probability of detecting the effect): {power:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Running Your First A/B Test\n",
    "\n",
    "Let's walk through a complete example. Imagine we're testing a new checkout button design to see if it improves conversion rate.\n",
    "\n",
    "**Current (Control)**: Blue \"Buy Now\" button - 10% conversion rate  \n",
    "**New (Treatment)**: Green \"Complete Purchase\" button - we want to detect a 2% absolute improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the test configuration\n",
    "test = ABTest(\n",
    "    alpha=0.05,      # 5% significance level\n",
    "    power=0.8,       # 80% power\n",
    "    metric_type=MetricType.PROPORTION,  # Conversion rate is a proportion\n",
    "    test_type=TestType.TWO_SIDED        # We care about both increases and decreases\n",
    ")\n",
    "\n",
    "print(\"Test Configuration:\")\n",
    "print(f\"  Significance level (α): {test.alpha}\")\n",
    "print(f\"  Power (1-β): {test.power}\")\n",
    "print(f\"  Metric type: {test.metric_type.value}\")\n",
    "print(f\"  Test type: {test.test_type.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate required sample size\n",
    "baseline_rate = 0.10  # Current 10% conversion rate\n",
    "mde = 0.02           # Minimum detectable effect: 2 percentage points\n",
    "\n",
    "sample_size = test.get_sample_size(baseline_rate=baseline_rate, mde=mde)\n",
    "\n",
    "print(f\"\\nSample Size Calculation:\")\n",
    "print(f\"  Baseline conversion rate: {baseline_rate:.1%}\")\n",
    "print(f\"  Minimum detectable effect: {mde:.1%} (absolute)\")\n",
    "print(f\"  Expected treatment rate: {baseline_rate + mde:.1%}\")\n",
    "print(f\"  Relative lift: {mde/baseline_rate:.1%}\")\n",
    "print(f\"\\n  Required sample size per group: {sample_size:,}\")\n",
    "print(f\"  Total sample size: {sample_size * 2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Simulate the experiment\n",
    "# (In reality, you would collect real data)\n",
    "\n",
    "# True effect: treatment is actually better by 2.5 percentage points\n",
    "true_control_rate = 0.10\n",
    "true_treatment_rate = 0.125  # 12.5%\n",
    "\n",
    "# Simulate data collection\n",
    "n_per_group = sample_size\n",
    "\n",
    "control_conversions = np.random.binomial(n_per_group, true_control_rate)\n",
    "treatment_conversions = np.random.binomial(n_per_group, true_treatment_rate)\n",
    "\n",
    "print(f\"\\nSimulated Experiment Results:\")\n",
    "print(f\"  Control: {control_conversions:,} conversions out of {n_per_group:,} ({control_conversions/n_per_group:.2%})\")\n",
    "print(f\"  Treatment: {treatment_conversions:,} conversions out of {n_per_group:,} ({treatment_conversions/n_per_group:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze the results\n",
    "result = test.analyze_proportions(\n",
    "    control_conversions=control_conversions,\n",
    "    control_total=n_per_group,\n",
    "    treatment_conversions=treatment_conversions,\n",
    "    treatment_total=n_per_group\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize the results\n",
    "fig = test.plot_results()\n",
    "plt.show()\n",
    "\n",
    "# Print full summary\n",
    "print(test.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Interpreting Results\n",
    "\n",
    "### What the Numbers Mean\n",
    "\n",
    "1. **P-value**: The probability of seeing a difference this large (or larger) if there really were no difference. \n",
    "   - p < 0.05 → \"Statistically significant\" (reject H₀)\n",
    "   - p ≥ 0.05 → \"Not statistically significant\" (don't reject H₀)\n",
    "\n",
    "2. **Confidence Interval (CI)**: A range of plausible values for the true effect.\n",
    "   - If CI doesn't include 0, the result is significant\n",
    "   - Wider CI = more uncertainty\n",
    "\n",
    "3. **Lift**: The difference between treatment and control.\n",
    "   - Absolute lift: Treatment rate - Control rate\n",
    "   - Relative lift: (Treatment rate - Control rate) / Control rate\n",
    "\n",
    "### Important: Statistical vs Practical Significance\n",
    "\n",
    "A result can be statistically significant but practically meaningless:\n",
    "- With a huge sample, even tiny differences become \"significant\"\n",
    "- Always consider: Is this effect large enough to matter for the business?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating statistical vs practical significance\n",
    "# Tiny effect with huge sample size\n",
    "\n",
    "huge_test = ABTest()\n",
    "huge_result = huge_test.analyze_proportions(\n",
    "    control_conversions=100000,\n",
    "    control_total=1000000,\n",
    "    treatment_conversions=100500,  # Only 0.05% absolute difference\n",
    "    treatment_total=1000000\n",
    ")\n",
    "\n",
    "print(\"Example: Statistically Significant but Practically Meaningless\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Control rate: {huge_result.control_mean:.4%}\")\n",
    "print(f\"Treatment rate: {huge_result.treatment_mean:.4%}\")\n",
    "print(f\"Absolute difference: {huge_result.absolute_lift:.4%}\")\n",
    "print(f\"P-value: {huge_result.p_value:.4f}\")\n",
    "print(f\"Significant: {huge_result.is_significant}\")\n",
    "print(f\"\\nBut wait... is a 0.05% improvement worth implementing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Common Pitfalls to Avoid\n",
    "\n",
    "### Pitfall 1: Peeking at Results Too Early\n",
    "Looking at results multiple times and stopping when you see significance inflates your false positive rate.\n",
    "\n",
    "### Pitfall 2: Stopping Too Early\n",
    "Running until you get a significant result guarantees you'll eventually get one, even if there's no real effect.\n",
    "\n",
    "### Pitfall 3: Sample Ratio Mismatch (SRM)\n",
    "If you expect a 50/50 split but get 55/45, something is wrong with your randomization.\n",
    "\n",
    "### Pitfall 4: Multiple Comparisons\n",
    "Testing many metrics increases false positive rate. If you test 20 metrics at α=0.05, you expect 1 false positive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the peeking problem\n",
    "from experiments import detect_srm\n",
    "\n",
    "print(\"Pitfall 3: Sample Ratio Mismatch Detection\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Good: No SRM\n",
    "srm_good = detect_srm(n_control=5000, n_treatment=5100)\n",
    "print(f\"\\nBalanced split (5000 vs 5100):\")\n",
    "print(f\"  Expected ratio: {srm_good.expected_ratio}\")\n",
    "print(f\"  Observed ratio: {srm_good.observed_ratio:.3f}\")\n",
    "print(f\"  P-value: {srm_good.p_value:.4f}\")\n",
    "print(f\"  SRM detected: {srm_good.is_mismatch}\")\n",
    "\n",
    "# Bad: SRM detected\n",
    "srm_bad = detect_srm(n_control=5000, n_treatment=6000)\n",
    "print(f\"\\nUnbalanced split (5000 vs 6000):\")\n",
    "print(f\"  Expected ratio: {srm_bad.expected_ratio}\")\n",
    "print(f\"  Observed ratio: {srm_bad.observed_ratio:.3f}\")\n",
    "print(f\"  P-value: {srm_bad.p_value:.6f}\")\n",
    "print(f\"  SRM detected: {srm_bad.is_mismatch} ⚠️ INVESTIGATE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Practice Exercises\n",
    "\n",
    "### Exercise 1: Calculate Sample Size\n",
    "You want to test a new email subject line. Your current open rate is 20%, and you want to detect a 3% absolute improvement.\n",
    "\n",
    "**Question**: How many emails do you need to send per group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "email_test = ABTest()\n",
    "\n",
    "# TODO: Calculate sample size for email open rate test\n",
    "# sample_size = email_test.get_sample_size(...)\n",
    "# print(f\"Required sample size per group: {sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for solution</summary>\n",
    "\n",
    "```python\n",
    "email_test = ABTest()\n",
    "sample_size = email_test.get_sample_size(baseline_rate=0.20, mde=0.03)\n",
    "print(f\"Required sample size per group: {sample_size}\")\n",
    "# Answer: approximately 2,143 per group\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Analyze Results\n",
    "You ran a test with the following results:\n",
    "- Control: 450 conversions out of 4,500 visitors\n",
    "- Treatment: 520 conversions out of 4,600 visitors\n",
    "\n",
    "**Question**: Is the treatment significantly better? What's the lift?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "exercise_test = ABTest()\n",
    "\n",
    "# TODO: Analyze the results\n",
    "# result = exercise_test.analyze_proportions(...)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Key Takeaways\n",
    "\n",
    "1. **A/B testing is a statistical framework** for comparing two options with proper randomization\n",
    "\n",
    "2. **Plan your test before starting**: Calculate sample size, define success criteria\n",
    "\n",
    "3. **P-value < 0.05 doesn't mean \"important\"**: Consider practical significance\n",
    "\n",
    "4. **Avoid common pitfalls**: Don't peek, check for SRM, account for multiple comparisons\n",
    "\n",
    "5. **Confidence intervals are your friend**: They show the range of plausible effects\n",
    "\n",
    "## Further Reading\n",
    "- Next notebook: `02_sample_size_power.ipynb` - Deep dive into power analysis\n",
    "- `06_experiment_diagnostics.ipynb` - How to validate your experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
